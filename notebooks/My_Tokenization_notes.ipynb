{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Text pre-processing for Machine Learning Algorithms\n",
    "\n",
    "Text cannot be used directly for any Text-processing or NLP algorithms. Text need to be preprossed so that it become suitable for the algorithms. The Bag of the Words (BoW) model is used in machine learning. BoW focuses on the occurance of words rather than the order. This can be done by assigning each word a unique number and then any text can be encoded with the fixed size vector of known words. For example\n",
    "\n",
    "    Vocab -           am, new, boy, a, and....\n",
    "    Assigned Number - 1,  2,   3,  4,   5, ..........\n",
    "    Intial Vector   { 0 , 0 , 0 , 0 , 0 , 0.....}\n",
    "\n",
    "    Now the text - I am a boy and I am new\n",
    "    In this text, 'I' has occured once. Similary 'am', 'a', 'boy'. So, 'I' has occured twice so in the vector the first position will be incremented to 2. Same thing is done for other words\n",
    "             am  new  boy  a  and  ....\n",
    "    Vector {  2,  1,    1,  1,  1 }\n",
    "\n",
    "In reality, the vocab is too big and many times there are many 0s in the vector. The scikit-learn library provides 3 different schemes that we can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Word Counts with CountVectorizer\n",
    "\n",
    "### Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "You can use it as follows:\n",
    "\n",
    "    1) Create an instance of the CountVectorizer class.\n",
    "    2) Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "    3) Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "    \n",
    " Because these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am': 0, 'boy': 2, 'and': 1, 'new': 3}\n",
      "(1, 4)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[2 1 1 1]]\n",
      "[[1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"I am a boy and I am new.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "vector2 = vectorizer.transform([\"I am a good boy\"])  #as \"good\" is not in vocab it will be ignored\n",
    "\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Word Frequencies with TfidfVectorizer (Term Frequency – Inverse Document)\n",
    "\n",
    "TfidfVectorizer is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf\n",
    "\n",
    "Sometimes the words like 'the', 'and' etc are repeated lot of times. This results in increasing number of counts\n",
    "\n",
    "    Term Frequency: As the name suggests, it counts the frequnecy of a word occurrance. \n",
    "    Inverse Document Frequency: Some words like 'the' ,'am', 'and' are so common and usally repeates very often. So this words are not as important and can be downsampled. This helped to focus on important words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Hashing with HashingVectorizer\n",
    "\n",
    "The limitation of the above two methods is that, it requires to generate vocab. Sometimes the vocab become unecessary long and hence is overhead. The alternate efficient method is by hashing. By hashing, the words are stored as integers hence eliminating the need of vocab. The drawback of this method is, the hashed word cannot be converted back to the original one. \n",
    "\n",
    "In below example, an aribitary vector length of 40 was choosen. The length should be set with atmost care to avoid frquent hash collisions. There are some heurestics which can be used to set the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40)\n",
      "[[-0.37796447  0.          0.          0.          0.         -0.37796447\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.37796447\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.75592895\n",
      "   0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"I am a bow and I am new\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=40)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
